# Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5, 2019)

**Authors:** Colin Raffel et al.  
**Link:** https://arxiv.org/abs/1910.10683

## Why this paper matters

T5 introduced a unifying perspective on NLP tasks by framing everything as text-to-text. Instead of designing task-specific heads or architectures, all tasks are expressed as converting one text sequence into another.

This simplification had a lasting influence on how people think about model interfaces and transfer learning.

## Core ideas

### Text-to-text formulation

Every task, including classification, translation, summarization, and question answering, is represented as a text input mapped to a text output. Labels become words, and task structure is communicated through prompts.

This removes architectural differences between tasks and shifts complexity into data and prompting.

### Encoder–decoder pretraining

T5 uses an encoder–decoder Transformer trained with a denoising objective. Portions of the input are masked and replaced with sentinel tokens, and the model learns to reconstruct the missing spans.

This objective encourages both strong representations and flexible generation.

### Transfer learning at scale

The paper explores how performance scales with model size, dataset size, and task diversity. Large pre-trained models transfer well across a wide range of tasks with minimal fine-tuning.

This reinforced pretraining as the dominant paradigm in NLP.

## Results

T5 achieves strong performance across the GLUE benchmark and many other NLP tasks. The text-to-text approach proves competitive with specialized models while being simpler to extend.

The results show that architectural uniformity does not limit performance.

## Impact

T5 influenced later work on prompting, multitask training, and instruction tuning. The idea that tasks can be specified in natural language rather than code became central to how people interact with language models.

Many later systems implicitly adopt the text-to-text mindset even when using decoder-only architectures.

## Modern perspective

While most frontier LLMs today are decoder-only, the conceptual contribution of T5 remains important. The paper clarified that task formulation and interface design can matter as much as model internals.

T5 also provides a useful contrast to autoregressive-only approaches.

## Notes

This paper pairs well with BERT and GPT-3. Together, they illustrate three different answers to the question of how a general-purpose language model should be trained and used.
